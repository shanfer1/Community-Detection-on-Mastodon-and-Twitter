{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KJCfpdxQpJ0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('proVaccineTweets.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df))\n",
        "# Get the list of column names\n",
        "column_names = df.columns.tolist()\n",
        "\n",
        "# Print the column names\n",
        "print(column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3BbtG-aQv63",
        "outputId": "0443fa48-dc98-46ce-942c-0b02001b684a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n",
            "['Unnamed: 0', 'username', 'created_at', 'truncated', 'description', 'following', 'followers', 'totaltweets', 'retweetcount', 'text', 'hashtags', 'user_mention', 'retweetScreenNames']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# load data from csv file\n",
        "df = pd.read_csv('proVaccineTweets.csv')\n",
        "\n",
        "# remove newline characters and links from text\n",
        "df['clean_text'] = df['text'].apply(lambda x: re.sub(r'[\\n]+', ' ', x))\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
        "\n",
        "# print some examples of cleaned text\n",
        "for i in range(10):\n",
        "    print(df['username'][i], df['clean_text'][i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY1WsUnbQxeI",
        "outputId": "23d8d82a-2076-4888-d80c-e4769e7f66f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hephaistos_ai Has anyone tried getting a #Novavax primary series 9 months+ out from a first booster? #COVID #GetVaccinated\n",
            "this1wierdgirl Has anyone tried getting a #Novavax primary series 9 months+ out from a first booster? #COVID #GetVaccinated\n",
            "alisonbuckley @YellowstoneMTCA Good to hear! Hope it keeps you healthy. Just got the Moderna bivalent/C19 vx plus double strength flu vx today. To summarize I have had the following shots in 18mos: 3/Pfizer c19 2/Moderna - c19 and BV C19 2021 flu shot 2 shingles shots 2022 flu shot Up to 9! #VaccinesWork\n",
            "StandleyLaurel I got the bivalent booster this morning. I’m a little achy but glad for the added protection. #VaccinesWork\n",
            "BWellsMC There. Is. No. Such. Thing. As. Mild. Covid.  Each infection weakens and worsens you. It DOES NOT MAKE YOU STRONGER.  #CovidIsNotOver  #CovidisAirborne  #MaskUp  #GetVaccinated  #GetBoosted  #AirFiltration \n",
            "Laurel_Standley I got the bivalent booster this morning. I’m a little achy but glad for the added protection. #VaccinesWork\n",
            "toothdoc3 The COVID-19 #MaskMandate and #VaccineMandate imposed on the federal #HeadStart program was struck down by a federal judge who said the mandate is clearly outside the power of the agency that promulgated and enforced it. \n",
            "JasonVhees13 The COVID-19 #MaskMandate and #VaccineMandate imposed on the federal #HeadStart program was struck down by a federal judge who said the mandate is clearly outside the power of the agency that promulgated and enforced it. \n",
            "tres444 The COVID-19 #MaskMandate and #VaccineMandate imposed on the federal #HeadStart program was struck down by a federal judge who said the mandate is clearly outside the power of the agency that promulgated and enforced it. \n",
            "skiermichael To folks who didn’t get the covid19 vaccine cause they don’t want to be part of a clinical trial for a drug lacking full approval.  They are part of the trial. They are the control group and are dying at a rate 20 times higher than those with the vaccine. #VaccinesWork\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Load data from Pandas dataframe\n",
        "\n",
        "# Tokenization\n",
        "df['tokens'] = df['clean_text'].apply(word_tokenize)\n",
        "\n",
        "# Stopword removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
        "\n",
        "# Stemming or lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "\n",
        "# Part-of-speech tagging\n",
        "df['pos_tags'] = df['tokens'].apply(pos_tag)\n",
        "\n",
        "# Named entity recognition\n",
        "df['ner_tags'] = df['pos_tags'].apply(ne_chunk)\n",
        "\n",
        "# Co-occurrence matrix\n",
        "vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
        "matrix = vectorizer.fit_transform(df['tokens'].apply(lambda x: \" \".join(x)))\n",
        "cooccurrence_matrix = (matrix.T * matrix)\n",
        "\n",
        "# TF-IDF weighting\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['tokens'].apply(lambda x: \" \".join(x)))\n",
        "\n",
        "df.to_csv(\"preprocessed_tiwtter_pro_data.csv\")\n",
        "print(\"done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6WmnCYASb6S",
        "outputId": "a5c39eb1-0755-4c7a-8191-4b5309bb46ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Create directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes for usernames and retweet screen names\n",
        "for user in set(df['username']) | set(df['retweetScreenNames']):\n",
        "    G.add_node(user)\n",
        "\n",
        "# Add edges for retweets\n",
        "for i, row in df.iterrows():\n",
        "    if row['retweetScreenNames'] != '':\n",
        "        G.add_edge(row['username'], row['retweetScreenNames'])\n",
        "\n",
        "# Save graph to file\n",
        "nx.write_graphml(G, \"twitter_graph.graphml\")"
      ],
      "metadata": {
        "id": "FFdJXeZcTr-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_neg = pd.read_csv(\"antiVaccineTweets.csv\")\n",
        "\n",
        "\n",
        "print(len(df_neg))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Get the list of column names\n",
        "column_names = df_neg.columns.tolist()\n",
        "\n",
        "# Print the column names\n",
        "print(column_names)\n",
        "\n",
        "\n",
        "unique_usernames = df_neg['username'].nunique()\n",
        "print(\"Number of unique usernames:\", unique_usernames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAjZwWAHnDzU",
        "outputId": "04012ba4-da15-44a7-c776-a9dabb73a331"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n",
            "['Unnamed: 0', 'username', 'created_at', 'truncated', 'description', 'following', 'followers', 'totaltweets', 'retweetcount', 'text', 'hashtags', 'user_mention', 'retweetScreenNames']\n",
            "Number of unique usernames: 167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "\n",
        "# remove newline characters and links from text\n",
        "df_neg['clean_text'] = df_neg['text'].apply(lambda x: re.sub(r'[\\n]+', ' ', x))\n",
        "df_neg['clean_text'] = df_neg['clean_text'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
        "\n",
        "# print some examples of cleaned text\n",
        "for i in range(10):\n",
        "    print(df_neg['username'][i], df_neg['clean_text'][i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH-zYwj8nsXU",
        "outputId": "8e07344f-6cde-44cc-fd89-f096b08e7230"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kittyhundal Needed: Joint @INTERPOL_HQ-RCMP-FBI Investigation into Gov Corruption/Fraud/Racketeering #cdnpoli #onpoli #bcpoli #qcpoli  #saskpoli  #NoVaccineMandates #NoVaccinePassports #TruckersForFreedom #TheGreatResist #Nuremberg2 #NoFarmsNoFood \n",
            "kittyhundal Needed: Joint @INTERPOL_HQ-RCMP-FBI Investigation into Gov Corruption/Fraud/Racketeering #cdnpoli #onpoli #bcpoli #qcpoli  #saskpoli  #NoVaccineMandates #NoVaccinePassports #TruckersForFreedom #TheGreatResist #Nuremberg2 #NoFarmsNoFood \n",
            "BellaLea88 Two weeks before the pandemic started, the government (NIAID) and MODERNA signed a confidential agreement regarding COVID VACCINES, how did they know!!?? Anthony Fauci has to answer some questions! #vaccine #vaccineinjury #vaccinedeaths \n",
            "HPot8888 Needed: Joint @INTERPOL_HQ-RCMP-FBI Investigation into Gov Corruption/Fraud/Racketeering #cdnpoli #onpoli #bcpoli #qcpoli  #saskpoli  #NoVaccineMandates #NoVaccinePassports #TruckersForFreedom #TheGreatResist #Nuremberg2 #NoFarmsNoFood \n",
            "HRMN8RHerm Attorney lawyer for military, Todd Callender:  “according to DMED, military database, all cause mortality spiked 1100% in just 10 months, the year the vaccine was introduced” That’s just the starting statement These facts are disturbing #vaccine #VaccineDeath #Vaccineinjury \n",
            "shortfinal32 @rpoconnor ...how did it go for those who recommended Thalidomide? #cdnpoli #NoVaccineMandates #CommunistCanada\n",
            "kittyhundal Needed: Joint @INTERPOL_HQ-RCMP-FBI Investigation into Gov Corruption/Fraud/Racketeering #cdnpoli #onpoli #bcpoli #qcpoli  #saskpoli  #NoVaccineMandates #NoVaccinePassports #TruckersForFreedom #TheGreatResist #Nuremberg2 #NoFarmsNoFood \n",
            "kittyhundal Needed: Joint @INTERPOL_HQ-RCMP-FBI Investigation into Gov Corruption/Fraud/Racketeering #cdnpoli #onpoli #bcpoli #qcpoli  #saskpoli  #NoVaccineMandates #NoVaccinePassports #TruckersForFreedom #TheGreatResist #Nuremberg2 #NoFarmsNoFood \n",
            "kittyhundal Needed: Joint @INTERPOL_HQ-RCMP-FBI Investigation into Gov Corruption/Fraud/Racketeering #cdnpoli #onpoli #bcpoli #qcpoli  #saskpoli  #NoVaccineMandates #NoVaccinePassports #TruckersForFreedom #TheGreatResist #Nuremberg2 #NoFarmsNoFood \n",
            "ChoicesMatter_ #vaccinedeath is real #vaccineinjury is real Young/healthy people falling over dead (post 💉 or otherwise) is NOT NORMAL! Young/healthy people dying in their sleep (post 💉 or otherwise) is NOT NORMAL! #StopTheShot #DoNotComply #stoptheshots #Informedconsent is a right!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Load data from Pandas dataframe\n",
        "\n",
        "# Tokenization\n",
        "df_neg['tokens'] = df_neg['clean_text'].apply(word_tokenize)\n",
        "\n",
        "# Stopword removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df_neg['tokens'] = df_neg['tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
        "\n",
        "# Stemming or lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df_neg['tokens'] = df_neg['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "\n",
        "# Part-of-speech tagging\n",
        "df_neg['pos_tags'] = df_neg['tokens'].apply(pos_tag)\n",
        "\n",
        "# Named entity recognition\n",
        "df_neg['ner_tags'] = df_neg['pos_tags'].apply(ne_chunk)\n",
        "\n",
        "# Co-occurrence matrix\n",
        "vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
        "matrix = vectorizer.fit_transform(df_neg['tokens'].apply(lambda x: \" \".join(x)))\n",
        "cooccurrence_matrix = (matrix.T * matrix)\n",
        "\n",
        "# TF-IDF weighting\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df_neg['tokens'].apply(lambda x: \" \".join(x)))\n",
        "\n",
        "df_neg.to_csv(\"preprocessed_tiwtter_anti_data.csv\")\n",
        "\n",
        "print(\"done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UivFpD0ooLGw",
        "outputId": "ba8ddbeb-13ae-4195-d5cf-ab474447a074"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Create directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes for usernames and retweet screen names\n",
        "for user in set(df_neg['username']) | set(df_neg['retweetScreenNames']):\n",
        "    G.add_node(user)\n",
        "\n",
        "# Add edges for retweets\n",
        "for i, row in df_neg.iterrows():\n",
        "    if row['retweetScreenNames'] != '':\n",
        "        G.add_edge(row['username'], row['retweetScreenNames'])\n",
        "\n",
        "# Save graph to file\n",
        "nx.write_graphml(G, \"twitter_graph_neg.graphml\")"
      ],
      "metadata": {
        "id": "Fid6FVXwogx_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6CFRtXMnyH3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mastodon Positive\n"
      ],
      "metadata": {
        "id": "jZ2JtSt_yIYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_positive_tweets = pd.read_csv('positive_tweets.csv')"
      ],
      "metadata": {
        "id": "Cwzi-neHyH5z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_positive_tweets))\n",
        "# Get the list of column names\n",
        "column_names = df_positive_tweets.columns.tolist()\n",
        "\n",
        "# Print the column names\n",
        "print(column_names)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpnuOHJhyH75",
        "outputId": "2b3a2068-e037-43f3-b63f-39aa0eab9ddd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1520\n",
            "['Unnamed: 0', 'user_id', 'username', 'content', 'hashtag', 'url', 'uri', 'reblogged_user_id', 'clean_text', 'tokens', 'pos_tags', 'ner_tags']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import ast\n",
        "\n",
        "# Create directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes for user ids and username as node attribute\n",
        "for i, row in df_positive_tweets.iterrows():\n",
        "    G.add_node(row['user_id'], username=row['username'])\n",
        "\n",
        "    # Add edges for reblogs\n",
        "    reblogged_user_ids = ast.literal_eval(row['reblogged_user_id'])\n",
        "\n",
        "    if reblogged_user_ids:\n",
        "        for reblogged_user_id in reblogged_user_ids:\n",
        "            G.add_edge(reblogged_user_id, row['user_id'])\n",
        "\n",
        "# Save graph to file\n",
        "nx.write_graphml(G, \"positive_tweets_mastodon_graph.graphml\")\n"
      ],
      "metadata": {
        "id": "AcVzSmnS3DFf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_negative_tweets = pd.read_csv('combined_negative_tweets.csv')"
      ],
      "metadata": {
        "id": "reMJmjhIesER"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_negative_tweets))\n",
        "# Get the list of column names\n",
        "column_names = df_negative_tweets.columns.tolist()\n",
        "\n",
        "# Print the column names\n",
        "print(column_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lMaUeste-ZB",
        "outputId": "31bf0c43-506c-48e1-ac5a-ed76dd7114ef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "287\n",
            "['Unnamed: 0', 'user_id', 'username', 'content', 'hashtag', 'url', 'uri', 'reblogged_user_id', 'clean_text', 'tokens', 'pos_tags', 'ner_tags']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import ast\n",
        "\n",
        "# Create directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes for user ids and username as node attribute\n",
        "for i, row in df_negative_tweets.iterrows():\n",
        "    G.add_node(row['user_id'], username=row['username'])\n",
        "\n",
        "    # Add edges for reblogs\n",
        "    reblogged_user_ids = ast.literal_eval(row['reblogged_user_id'])\n",
        "\n",
        "    if reblogged_user_ids:\n",
        "        for reblogged_user_id in reblogged_user_ids:\n",
        "            G.add_edge(reblogged_user_id, row['user_id'])\n",
        "\n",
        "# Save graph to file\n",
        "nx.write_graphml(G, \"negative_tweets_mastodon_graph.graphml\")\n"
      ],
      "metadata": {
        "id": "kUhPCC-IfU7X"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}