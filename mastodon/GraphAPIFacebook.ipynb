{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y66PhErnDBu5"
      },
      "outputs": [],
      "source": [
        "#get the data \n",
        "\n",
        "import pandas as pd\n",
        "import requests # to fetch data \n",
        "import re # to use regular expressions on strings\n",
        "import pandas as pd # to show final results in a table\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd1QOW5Y2roa",
        "outputId": "229d05c3-815b-488b-d266-9449ba717a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqbN3aM82yCS"
      },
      "outputs": [],
      "source": [
        "#the data coming from the drive is : id , uri, url ,content , account[username], account['id']\n",
        "columns = ['user_id', 'username','content','url','uri']\n",
        "\n",
        "# Create an empty DataFrame with the defined column names\n",
        "df = pd.DataFrame(columns=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eltlqblWBmoj",
        "outputId": "c49ccb48-d527-4f8b-ab23-b4908b1514bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffGetVaccinated', 'GetVaxxed', 'VaxPlus', 'VaccineMandate', 'GetVaccinatedNow', 'MandatoryVaccination', 'VaccinesWork', 'VaccinationWork', 'FullyVaccinated', 'TrustTheScience', 'VaccinesSaveLives', 'VaccinatedForCovid', 'AntivacinIdiots', 'ProScience', 'GetVaccinatedOrGetCovid', 'Antivaxxers', 'CovidIsNotOver', 'GetVaxxedForTheKids', 'Getvaccinatedtobeprotected', 'GetVaccinatedtoProtectKids', 'GetVaxxedRightNow', 'GetVaccinatedASAP', 'GetYourBooster', 'GetVaxxedtoTheMax', 'ProtectYourselfAndYourCommunity', 'NoVaccine', 'NoVaccineMandates', 'SayNoToVaccineMandate', 'NoVaxMandates', 'FuckTheVax', 'FuckVaccines', 'AntiVaccine', 'NoForcedVaccines', 'Notomandatoryvaccines', 'NoVaccineMandates', 'NoVaccineForMe', 'SayNoToVaccines', 'CovidVaccineIsPoison', 'VaccinesArePoison', 'VaccineDamage', 'VaccineFailure', 'VaccineHarm', 'VaccineInjury', 'VaccinesAreNotTheAnswer', 'Vaccineskill', 'NoVaccinePassports', 'AntivaxxChronicles', 'StopVaccination', 'NoVaccinePassportsAnywhere', 'VaccineSideEffects', 'NoBooster', 'NoVaccine_NoPandemic', 'DoNotComply']\n",
            "53\n"
          ]
        }
      ],
      "source": [
        "#prepare hashtags:\n",
        "\n",
        "\n",
        "filename = 'hashtags.txt'  # Replace with your desired filename\n",
        "hashtags = []  # Initialize an empty list to store the lines\n",
        "\n",
        "# Open the file and read its contents line by line\n",
        "with open(filename, 'r',encoding='utf-8') as f:\n",
        "    for line in f.readlines():\n",
        "        # Strip whitespace from the line and split it into a list of words\n",
        "        words = line.strip().split()\n",
        "        \n",
        "        # Join the words in the list with commas and append the resulting string to the list\n",
        "        hashtags.append(''.join(words))\n",
        "\n",
        "# Print the resulting list\n",
        "print(hashtags)\n",
        "\n",
        "\n",
        "\n",
        "# hashtags=['NoVaccine','NoVaccineMandates','SayNoToVaccineMandate','NoVaxMandates','FuckTheVax','FuckVaccines','AntiVaccine','NoForcedVaccines','GetVaccinated','GetVaxxed','VaxPlus','VaccineMandate','GetVaccinatedNow','MandatoryVaccination','VaccinesWork','VaccinationWork','FullyVaccinated','VaccinesSaveLives','VaccinatedForCovid','AntivacinIdiots','ProScience']\n",
        "print(len(hashtags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzIXPowLNeoF",
        "outputId": "0c209277-87ea-4096-e9bd-9eb1144c7bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['GetVaccinated', 'GetVaxxed', 'VaxPlus', 'VaccineMandate', 'GetVaccinatedNow', 'MandatoryVaccination', 'VaccinesWork', 'VaccinationWork', 'FullyVaccinated', 'TrustTheScience', 'VaccinesSaveLives', 'VaccinatedForCovid', 'AntivacinIdiots', 'ProScience', 'GetVaccinatedOrGetCovid', 'Antivaxxers', 'CovidIsNotOver', 'GetVaxxedForTheKids', 'Getvaccinatedtobeprotected', 'GetVaccinatedtoProtectKids', 'GetVaxxedRightNow', 'GetVaccinatedASAP', 'GetYourBooster', 'GetVaxxedtoTheMax', 'ProtectYourselfAndYourCommunity', 'NoVaccine', 'NoVaccineMandates', 'SayNoToVaccineMandate', 'NoVaxMandates', 'FuckTheVax', 'FuckVaccines', 'AntiVaccine', 'NoForcedVaccines', 'Notomandatoryvaccines', 'NoVaccineMandates', 'NoVaccineForMe', 'SayNoToVaccines', 'CovidVaccineIsPoison', 'VaccinesArePoison', 'VaccineDamage', 'VaccineFailure', 'VaccineHarm', 'VaccineInjury', 'VaccinesAreNotTheAnswer', 'Vaccineskill', 'NoVaccinePassports', 'AntivaxxChronicles', 'StopVaccination', 'NoVaccinePassportsAnywhere', 'VaccineSideEffects', 'NoBooster', 'NoVaccine_NoPandemic', 'DoNotComply']\n"
          ]
        }
      ],
      "source": [
        "hashtags[0] = hashtags[0].lstrip('\\ufeff')[:]\n",
        "print(hashtags )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dSATffAtIxNQ"
      },
      "outputs": [],
      "source": [
        "columns = ['user_id', 'username','content','hashtag','url','uri']\n",
        "import pandas as pd\n",
        "# Create an empty DataFrame with the defined column names\n",
        "df_new = pd.DataFrame(columns=columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld9oe80hHQnU",
        "outputId": "751e1f0b-dc31-49cf-e692-ca9984c96f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 570 statuses\n",
            "Retrieved 187 statuses\n",
            "Retrieved 3 statuses\n",
            "Retrieved 53 statuses\n",
            "Retrieved 20 statuses\n",
            "Retrieved 49 statuses\n",
            "Retrieved 477 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 7 statuses\n",
            "Retrieved 27 statuses\n",
            "Retrieved 230 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 8 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 298 statuses\n",
            "Retrieved 8420 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 1 statuses\n",
            "Retrieved 3 statuses\n",
            "Retrieved 45 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 2 statuses\n",
            "Retrieved 24 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 1 statuses\n",
            "Retrieved 38 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 24 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 2 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 13 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 3 statuses\n",
            "Retrieved 66 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 3 statuses\n",
            "Retrieved 43 statuses\n",
            "Retrieved 18 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 0 statuses\n",
            "Retrieved 31 statuses\n",
            "Total data length 10666\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        " # Initialize an empty list to store the retrieved statuses\n",
        "for hashtag in hashtags:\n",
        "  limit = 80  # Set the maximum number of statuses to retrieve per page\n",
        "  max_id = None  # Initialize the max_id parameter to None\n",
        "  statuses = [] \n",
        "  # hashtag='getVaccinated'\n",
        "\n",
        "  while True:\n",
        "        # Construct the API URL with the current max_id value and the limit parameter\n",
        "        url = f\"https://mastodon.social/api/v1/timelines/tag/{hashtag}?limit={limit}\"\n",
        "        if max_id is not None:\n",
        "            url += f\"&max_id={max_id}\"\n",
        "        \n",
        "        # Make the API call and retrieve the statuses\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            new_statuses = response.json()\n",
        "            if len(new_statuses) == 0:\n",
        "                # No more statuses to retrieve\n",
        "                break\n",
        "            else:\n",
        "                # Append the retrieved statuses to the list\n",
        "                statuses += new_statuses\n",
        "                max_id = new_statuses[-1]['id']  # Update the max_id parameter for the next page\n",
        "        else:\n",
        "            # API call failed\n",
        "            print(f\"API call failed with status code {response.status_code}\")\n",
        "            break\n",
        "  print(f\"Retrieved {len(statuses)} statuses\")\n",
        "\n",
        "  # Print the number of retrieved statuses\n",
        "  for status in statuses:\n",
        "    assert status['visibility']=='public'\n",
        "    username=status['account']['username']\n",
        "    user_id=status['account']['id']\n",
        "    content=status['content']\n",
        "    url=status['url']\n",
        "    uri=status['uri']\n",
        "    # print(len(df_new))\n",
        "    df_new.loc[len(df_new.index)] = [user_id, username,content,hashtag,url,uri,]\n",
        "print(\"Total data length\",len(df_new))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "\n",
        "# Define a function to remove HTML tags\n",
        "def remove_tags(text):\n",
        "    clean_text = re.sub('<.*?>', '', text)\n",
        "    return clean_text\n",
        "\n",
        "# Apply the function to each row of the DataFrame\n",
        "df_new['clean_text'] = df_new['content'].apply(remove_tags)\n",
        "\n",
        "# Print the result\n",
        "print(df_new['clean_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq8zkUN0vlE-",
        "outputId": "322523d5-cd7a-4bd4-c060-083c8bad3b14"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        Just saw a toot about polio levels on wastewat...\n",
            "1        As David Gorski notes, the CDC report on autis...\n",
            "2        Show up for Montana Families for Vaccines Day ...\n",
            "3        Here's how you can lower your risk of getting ...\n",
            "4        Here are the earlier four, for completists. :)...\n",
            "                               ...                        \n",
            "10661    Is #antifa marching in the streets and flippin...\n",
            "10662    @thatguyoverthere he is not lucid. Someone els...\n",
            "10663    RT @Flwrgirl66x@twitter.comHow is this not alr...\n",
            "10664    RT @LPNH@twitter.comWhen you accept mandates, ...\n",
            "10665    These policy enforcers do not have authority o...\n",
            "Name: clean_text, Length: 10666, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNC6JPi50rjG",
        "outputId": "00c5a0e5-74eb-4db7-d9d6-b5e7d517b74f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load data from Pandas dataframe\n",
        "\n",
        "# Tokenization\n",
        "df_new['tokens'] = df_new['clean_text'].apply(word_tokenize)\n",
        "\n",
        "# Stopword removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df_new['tokens'] = df_new['tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
        "\n",
        "# Stemming or lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df_new['tokens'] = df_new['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "\n",
        "# Part-of-speech tagging\n",
        "df_new['pos_tags'] = df_new['tokens'].apply(pos_tag)\n",
        "\n",
        "# Named entity recognition\n",
        "df_new['ner_tags'] = df_new['pos_tags'].apply(ne_chunk)\n",
        "\n",
        "# Co-occurrence matrix\n",
        "vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
        "matrix = vectorizer.fit_transform(df_new['tokens'].apply(lambda x: \" \".join(x)))\n",
        "cooccurrence_matrix = (matrix.T * matrix)\n",
        "\n",
        "# TF-IDF weighting\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df_new['tokens'].apply(lambda x: \" \".join(x)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgXOZ7TgyLeb",
        "outputId": "43754645-15a9-426f-ed4e-6e8e8406533c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A0N2xqsCIbRH"
      },
      "outputs": [],
      "source": [
        "df_new.to_csv(\"final_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnF6S-O0IedY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}